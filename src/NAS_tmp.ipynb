{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from audioop import rms\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from itertools import product\n",
    "from time import localtime, sleep, strftime, time\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import setproctitle # to set the name of process\n",
    "import torch\n",
    "import torch.utils\n",
    "from tensorboardX import SummaryWriter \n",
    "from torch import multiprocessing as mp # 多线程工作\n",
    "\n",
    "from dataset import get_data_queue_cf, get_data_queue_cf_nonsparse, get_data_queue_efficiently, get_data_queue_negsampling_efficiently\n",
    "from models import (CML, DELF, DMF, FISM, GMF, MLP, SVD, JNCF_Cat, JNCF_Dot, SVD_plus_plus, SPACE, BaseModel, Virtue_CF)\n",
    "from controller import sample_arch_cf, sample_arch_cf_signal, sample_arch_cf_test\n",
    "from train_eval import (evaluate_cf, evaluate_cf_efficiently, evaluate_cf_efficiently_implicit, get_arch_performance_cf_signal_param_device, get_arch_performance_single_device, train_single_cf, train_single_cf_efficiently,get_arch_performance_implicit_single_device)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import GPUtil\n",
    "import socket\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Run.\")\n",
    "parser.add_argument('--lr', type=float, default=0.05, help='init learning rate')\n",
    "parser.add_argument('--arch_lr', type=float, default=0.05, help='learning rate for arch encoding')\n",
    "parser.add_argument('--controller_lr', type=float, default=1e-1, help='learning rate for controller')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-5, help='weight decay')\n",
    "parser.add_argument('--update_freq', type=int, default=1, help='frequency of updating architeture')\n",
    "parser.add_argument('--opt', type=str, default='Adagrad', help='choice of opt')\n",
    "parser.add_argument('--use_gpu', type=int, default=1, help='whether use gpu')\n",
    "parser.add_argument('--minibatch', type=int, default=1, help='whether use minibatch')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--train_epochs', type=int, default=2000, help='num of training epochs')\n",
    "parser.add_argument('--search_epochs', type=int, default=1000, help='num of searching epochs')\n",
    "parser.add_argument('--save', type=str, default='save/', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "parser.add_argument('--train_portion', type=float, default=0.5, help='portion of training data')\n",
    "parser.add_argument('--valid_portion', type=float, default=0.25, help='portion of validation data')\n",
    "parser.add_argument('--dataset', type=str, default='ml-100k', help='dataset')\n",
    "parser.add_argument('--mode', type=str, default='random_single', help='search or single mode')\n",
    "parser.add_argument('--process_name', type=str, default='AutoCF@wenyan', help='process name')\n",
    "parser.add_argument('--embedding_dim', type=int, default=2, help='dimension of embedding')\n",
    "parser.add_argument('--controller', type=str, default='PURE', help='structure of controller')\n",
    "parser.add_argument('--controller_batch_size', type=int, default=4, help='batch size for updating controller')\n",
    "parser.add_argument('--unrolled', action='store_true', default=True, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--max_batch', type=int, default=65536, help='max batch during training')\n",
    "parser.add_argument('--device', type=int, default=0, help='GPU device')\n",
    "parser.add_argument('--multi', type=int, default=0, help='using multi-training for single architecture')\n",
    "parser.add_argument('--if_valid', type=int, default=1, help='use validation set for tuning single architecture or not')\n",
    "parser.add_argument('--breakpoint', type=str, default='save/log.txt', help='the log file storing existing results')\n",
    "parser.add_argument('--arch_file', type=str, default='src/arch.txt', help='all arches')\n",
    "parser.add_argument('--remaining_arches', type=str, default='src/arch.txt', help='')\n",
    "parser.add_argument('--arch_assign', type=str, default='[0,3]', help='')\n",
    "parser.add_argument('--data_type', type=str, default='implicit', help='explicit or implicit(default)')\n",
    "parser.add_argument('--loss_func', type=str, default='bprloss', help='Implicit loss function')\n",
    "parser.add_argument('--mark', type=str, default='') # \n",
    "\n",
    "args = parser.parse_args([])\n",
    "mp.set_start_method('spawn', force=True) # 一种多任务运行方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start = time.time()\n",
    "dim = 2\n",
    "data_path = args.dataset + '/'\n",
    "\n",
    "# setting datasets,  default='ml-100k'\n",
    "if args.dataset == 'ml-100k': # default\n",
    "    num_users = 943\n",
    "    num_items = 1682\n",
    "elif args.dataset == 'ml-1m':\n",
    "    num_users = 6040\n",
    "    num_items = 3952\n",
    "elif args.dataset == 'ml-10m':\n",
    "    num_users = 71567\n",
    "    num_items = 65133\n",
    "    # num_users = 715\n",
    "    # num_items = 653\n",
    "elif args.dataset == 'ml-20m':\n",
    "    num_users = 138493\n",
    "    num_items = 131262\n",
    "elif args.dataset == 'youtube_small':\n",
    "    num_ps = 600\n",
    "    num_qs = 14340\n",
    "    num_rs = 5\n",
    "    dim = 3\n",
    "elif args.dataset == 'youtube':\n",
    "    num_ps = 15088\n",
    "    num_qs = 15088\n",
    "    num_rs = 5\n",
    "    dim = 3\n",
    "elif args.dataset == 'amazon-book':\n",
    "    num_users = 11899\n",
    "    num_items = 16196\n",
    "elif args.dataset == 'yelp':\n",
    "    # num_users = 26829\n",
    "    # num_items = 20344\n",
    "    # 31668, item_set: 1237259\n",
    "    num_users = 31668 \n",
    "    num_items = 38048\n",
    "elif args.dataset == 'yelp2':\n",
    "    num_users = 15496\n",
    "    num_items = 12666\n",
    "# yelp generated from: https://www.kaggle.com/yelp-dataset/yelp-dataset\n",
    "elif args.dataset == 'yelp-10k':\n",
    "    num_users = 9357-1\n",
    "    num_items = 4299\n",
    "elif args.dataset == 'yelp-50k':\n",
    "    num_users = 42919-1\n",
    "    num_items = 9033\n",
    "elif args.dataset == 'yelp-100k':\n",
    "    num_users = 80388-1\n",
    "    num_items = 11223\n",
    "elif args.dataset == 'yelp-1m':\n",
    "    num_users = 551747-1\n",
    "    num_items = 28085\n",
    "elif args.dataset == 'yelp-10m':\n",
    "    num_users = 1483546-1\n",
    "    num_items = 90315\n",
    "elif args.dataset == 'yelp-all':\n",
    "    num_users = 1483546-1\n",
    "    num_items = 90315\n",
    "else:\n",
    "    pass\n",
    "args.num_users = num_users\n",
    "args.num_items = num_items\n",
    "\n",
    "if args.data_type == 'implicit': # 主要使用这一行，隐式推荐\n",
    "    train_queue_pair, valid_queue, test_queue = get_data_queue_negsampling_efficiently(data_path, args)\n",
    "else: # train queue，显式推荐\n",
    "    train_queue, valid_queue, test_queue = get_data_queue_efficiently(data_path, args)\n",
    "# print(train_queue)\n",
    "# logging.logging.info('prepare data finish! [%f]' % (time()-data_start))\n",
    "stored_arches = {} # log ging表示添加到记录中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(arch_assign='[0,3]', arch_file='src/arch.txt', arch_lr=0.05, breakpoint='save/log.txt', controller='PURE', controller_batch_size=4, controller_lr=0.1, data_type='implicit', dataset='ml-100k', device=0, embedding_dim=2, gpu=0, grad_clip=5, if_valid=1, loss_func='bprloss', lr=0.05, mark='', max_batch=65536, minibatch=1, mode='random_single', multi=0, num_items=1682, num_users=943, opt='Adagrad', process_name='AutoCF@wenyan', remaining_arches='src/arch.txt', save='save/', search_epochs=1000, seed=1, train_epochs=2000, train_portion=0.5, unrolled=True, update_freq=1, use_gpu=1, valid_portion=0.25, weight_decay=1e-05)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.arch_assign: [0,3]\n",
      "remaining_arches_encoding: ['ri_mat_mat_max_i', 'rr_mlp_mlp_concat_i', 'ri_mlp_mat_min_mlp']\n"
     ]
    }
   ],
   "source": [
    "search_start = time.time()\n",
    "performance = {}\n",
    "best_arch, best_rmse = None, 100000\n",
    "arch_batch_size = 1  \n",
    "args.search_epochs = min(args.search_epochs, SPACE) # SPACE=135\n",
    "\n",
    "remaining_arches_encoding = open(args.remaining_arches, 'r').readlines() # open the file of arch: remaining_arches='src/arch.txt'\n",
    "remaining_arches_encoding = list(map(lambda x: x.strip(), remaining_arches_encoding))\n",
    "args.arch_assign = '[0,3]'\n",
    "if not args.arch_assign:\n",
    "    # not played\n",
    "    remaining_arches_encoding = remaining_arches_encoding\n",
    "else:\n",
    "    print(\"args.arch_assign: {}\".format(args.arch_assign))\n",
    "    start, end = eval(args.arch_assign)\n",
    "    remaining_arches_encoding = remaining_arches_encoding[start:end]\n",
    "arch_count = 0\n",
    "print(\"remaining_arches_encoding: {}\".format(remaining_arches_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if arch_count >= len(remaining_arches_encoding):\n",
    "    # break\n",
    "    pass\n",
    "# sample an arch\n",
    "arch_encoding = remaining_arches_encoding[arch_count] # 对应：u/i.enc_u.emb_i.emb_interaction_pre\n",
    "arch_single = sample_arch_cf()\n",
    "# u/i.enc, u.emb, i.emb, interaction, pre\n",
    "arch_single['cf'], arch_single['emb']['u'], arch_single['emb']['i'], arch_single['ifc'], arch_single['pred'] = arch_encoding.split('_')\n",
    "arch_count += 1\n",
    "performance[str(arch_single)] = 0\n",
    "if len(performance) >= len(remaining_arches_encoding):\n",
    "    # break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hostname: abc\n"
     ]
    }
   ],
   "source": [
    "arch_start = time.time()\n",
    "avaliable_device_ids = [0,1,2,3]\n",
    "hostname = socket.gethostname()\n",
    "print(\"hostname: {}\".format(hostname))\n",
    "if hostname == 'rl3':\n",
    "    avaliable_device_ids = [0, 1, 2, 3]\n",
    "elif hostname == 'fib-dl':\n",
    "    avaliable_device_ids = [0, 1, 2, 3]\n",
    "elif hostname == 'fib-dl3':\n",
    "    avaliable_device_ids = [2,3,5]\n",
    "elif hostname == 'fib':\n",
    "    avaliable_device_ids = [0, 1, 2, 3]\n",
    "elif hostname =='rl2':\n",
    "    avaliable_device_ids = [0, 1, 2, 3]\n",
    "elif hostname == 'abc':\n",
    "    # avaliable_device_ids = [4,5,6,7]\n",
    "    avaliable_device_ids = [0,1,2,3,4,5,6,7]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "lr_candidates = [0.01, 0.02, 0.05, 0.1]\n",
    "rank_candidates = [2, 4, 8, 16]\n",
    "hyper_parameters = list(product(lr_candidates, rank_candidates))\n",
    "run_one_model = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "task_number per GPU: 4\n",
      "task_split: [0, 5, 10, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "avaliable_device_ids = GPUtil.getAvailable(order = 'first', limit = 8, maxLoad = 0.5, maxMemory = 0.2, includeNan=False, excludeID=[], excludeUUID=[])\n",
    "if hostname == 'fib':\n",
    "    avaliable_device_ids = [0, 1, 2, 3]\n",
    "elif hostname == 'abc':\n",
    "    # avaliable_device_ids = [4,5,6,7]\n",
    "    avaliable_device_ids = [0,1,2,3]\n",
    "    # avaliable_device_ids = [0,1,2,3,4,5,6,7]\n",
    "else:\n",
    "    pass\n",
    "avaliable_device_ids = GPUtil.getAvailable(order = 'first', limit = 8, maxLoad = 0.5, maxMemory = 0.2, includeNan=False, excludeID=[], excludeUUID=[])\n",
    "print(avaliable_device_ids)\n",
    "assigned_device_ids = avaliable_device_ids\n",
    "if run_one_model > 0:\n",
    "    # break\n",
    "    pass\n",
    "task_number = math.ceil(16 / len(avaliable_device_ids)) \n",
    "print(\"task_number per GPU: {}\".format(task_number))\n",
    "task_split = list(range(0, 16, len(avaliable_device_ids)))\n",
    "task_split.append(16)\n",
    "print(\"task_split: {}\".format(task_split))\n",
    "task_index = [list(range(task_split[i], task_split[i+1])) for i in range(task_number)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasks: [0, 1, 2, 3, 4]\n",
      "Stage1, p: <multiprocessing.pool.Pool state=RUN pool_size=5>\n",
      "5\n",
      "[{'cf': 'ri', 'emb': {'u': 'mat', 'i': 'mat'}, 'ifc': 'max', 'pred': 'i'}, 943, 1682, [], [tensor([373, 221, 863,  ...,  57, 790, 129]), tensor([264,   3, 482,  ..., 474, 287, 251]), tensor([5., 3., 5.,  ..., 5., 3., 5.]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], [tensor([  0,   0,   0,  ..., 942, 942, 942]), tensor([ 42, 128,  89,  ..., 187, 228, 126]), tensor([[5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], [tensor([388,  94, 885,  ..., 268, 915,  47]), tensor([413, 109,  86,  ..., 528, 560, 646]), tensor([ 515,  638,  728,  ...,  975, 1247,  848]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], Namespace(arch_assign='[0,3]', arch_file='src/arch.txt', arch_lr=0.05, breakpoint='save/log.txt', controller='PURE', controller_batch_size=4, controller_lr=0.1, data_type='implicit', dataset='ml-100k', device=0, embedding_dim=2, gpu=0, grad_clip=5, if_valid=1, loss_func='bprloss', lr=0.05, mark='', max_batch=65536, minibatch=1, mode='random_single', multi=0, num_items=1682, num_users=943, opt='Adagrad', process_name='AutoCF@wenyan', remaining_arches='src/arch.txt', save='save/', search_epochs=135, seed=1, train_epochs=2000, train_portion=0.5, unrolled=True, update_freq=1, use_gpu=1, valid_portion=0.25, weight_decay=1e-05), (0.01, 2), 0, 1]\n",
      "tasks: [5, 6, 7, 8, 9]\n",
      "Stage1, p: <multiprocessing.pool.Pool state=RUN pool_size=5>\n",
      "5\n",
      "[{'cf': 'ri', 'emb': {'u': 'mat', 'i': 'mat'}, 'ifc': 'max', 'pred': 'i'}, 943, 1682, [], [tensor([373, 221, 863,  ...,  57, 790, 129]), tensor([264,   3, 482,  ..., 474, 287, 251]), tensor([5., 3., 5.,  ..., 5., 3., 5.]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], [tensor([  0,   0,   0,  ..., 942, 942, 942]), tensor([ 42, 128,  89,  ..., 187, 228, 126]), tensor([[5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], [tensor([388,  94, 885,  ..., 268, 915,  47]), tensor([413, 109,  86,  ..., 528, 560, 646]), tensor([ 515,  638,  728,  ...,  975, 1247,  848]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], Namespace(arch_assign='[0,3]', arch_file='src/arch.txt', arch_lr=0.05, breakpoint='save/log.txt', controller='PURE', controller_batch_size=4, controller_lr=0.1, data_type='implicit', dataset='ml-100k', device=0, embedding_dim=2, gpu=0, grad_clip=5, if_valid=1, loss_func='bprloss', lr=0.05, mark='', max_batch=65536, minibatch=1, mode='random_single', multi=0, num_items=1682, num_users=943, opt='Adagrad', process_name='AutoCF@wenyan', remaining_arches='src/arch.txt', save='save/', search_epochs=135, seed=1, train_epochs=2000, train_portion=0.5, unrolled=True, update_freq=1, use_gpu=1, valid_portion=0.25, weight_decay=1e-05), (0.02, 4), 0, 1]\n",
      "tasks: [10, 11, 12, 13, 14]\n",
      "Stage1, p: <multiprocessing.pool.Pool state=RUN pool_size=5>\n",
      "5\n",
      "[{'cf': 'ri', 'emb': {'u': 'mat', 'i': 'mat'}, 'ifc': 'max', 'pred': 'i'}, 943, 1682, [], [tensor([373, 221, 863,  ...,  57, 790, 129]), tensor([264,   3, 482,  ..., 474, 287, 251]), tensor([5., 3., 5.,  ..., 5., 3., 5.]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], [tensor([  0,   0,   0,  ..., 942, 942, 942]), tensor([ 42, 128,  89,  ..., 187, 228, 126]), tensor([[5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], [tensor([388,  94, 885,  ..., 268, 915,  47]), tensor([413, 109,  86,  ..., 528, 560, 646]), tensor([ 515,  638,  728,  ...,  975, 1247,  848]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], Namespace(arch_assign='[0,3]', arch_file='src/arch.txt', arch_lr=0.05, breakpoint='save/log.txt', controller='PURE', controller_batch_size=4, controller_lr=0.1, data_type='implicit', dataset='ml-100k', device=0, embedding_dim=2, gpu=0, grad_clip=5, if_valid=1, loss_func='bprloss', lr=0.05, mark='', max_batch=65536, minibatch=1, mode='random_single', multi=0, num_items=1682, num_users=943, opt='Adagrad', process_name='AutoCF@wenyan', remaining_arches='src/arch.txt', save='save/', search_epochs=135, seed=1, train_epochs=2000, train_portion=0.5, unrolled=True, update_freq=1, use_gpu=1, valid_portion=0.25, weight_decay=1e-05), (0.05, 8), 0, 1]\n",
      "tasks: [15]\n",
      "Stage1, p: <multiprocessing.pool.Pool state=RUN pool_size=1>\n",
      "1\n",
      "[{'cf': 'ri', 'emb': {'u': 'mat', 'i': 'mat'}, 'ifc': 'max', 'pred': 'i'}, 943, 1682, [], [tensor([373, 221, 863,  ...,  57, 790, 129]), tensor([264,   3, 482,  ..., 474, 287, 251]), tensor([5., 3., 5.,  ..., 5., 3., 5.]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], [tensor([  0,   0,   0,  ..., 942, 942, 942]), tensor([ 42, 128,  89,  ..., 187, 228, 126]), tensor([[5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], [tensor([388,  94, 885,  ..., 268, 915,  47]), tensor([413, 109,  86,  ..., 528, 560, 646]), tensor([ 515,  638,  728,  ...,  975, 1247,  848]), tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 5., 0.,  ..., 0., 0., 0.]]), tensor([[0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])], Namespace(arch_assign='[0,3]', arch_file='src/arch.txt', arch_lr=0.05, breakpoint='save/log.txt', controller='PURE', controller_batch_size=4, controller_lr=0.1, data_type='implicit', dataset='ml-100k', device=0, embedding_dim=2, gpu=0, grad_clip=5, if_valid=1, loss_func='bprloss', lr=0.05, mark='', max_batch=65536, minibatch=1, mode='random_single', multi=0, num_items=1682, num_users=943, opt='Adagrad', process_name='AutoCF@wenyan', remaining_arches='src/arch.txt', save='save/', search_epochs=135, seed=1, train_epochs=2000, train_portion=0.5, unrolled=True, update_freq=1, use_gpu=1, valid_portion=0.25, weight_decay=1e-05), (0.1, 16), 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for tasks in task_index:\n",
    "    print(\"tasks: {}\".format(tasks))\n",
    "    with mp.Pool(processes=len(tasks)) as p:\n",
    "        print('Stage1, p: {}'.format(p))\n",
    "        p.name = 'test'\n",
    "        if args.data_type == 'implicit':# 装载\n",
    "            jobs = [[arch_single, num_users, num_items, [], valid_queue, test_queue, train_queue_pair, args, hyper_parameters[i], assigned_device_ids[i % len(assigned_device_ids)], args.if_valid] for i in tasks]\n",
    "        else:\n",
    "            jobs = [[arch_single, num_users, num_items, train_queue, valid_queue, test_queue, args, hyper_parameters[i], assigned_device_ids[i % len(assigned_device_ids)], args.if_valid] for i in tasks]\n",
    "        # rmse_list1 = p.map(get_single_model_performance, jobs)\n",
    "        print(len(jobs))\n",
    "        # print(jobs[0])\n",
    "        run_one_model += 1\n",
    "        p.close()\n",
    "# 此处循环有 16 个\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ri_mat_mat_max_i {'cf': 'ri', 'emb': {'u': 'mat', 'i': 'mat'}, 'ifc': 'max', 'pred': 'i'}\n",
      "{\"{'cf': 'ri', 'emb': {'u': 'mat', 'i': 'mat'}, 'ifc': 'max', 'pred': 'i'}\": 0}\n",
      "[(0.01, 2), (0.01, 4), (0.01, 8), (0.01, 16), (0.02, 2), (0.02, 4), (0.02, 8), (0.02, 16), (0.05, 2), (0.05, 4), (0.05, 8), (0.05, 16), (0.1, 2), (0.1, 4), (0.1, 8), (0.1, 16)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(arch_encoding, arch_single)\n",
    "print(performance)\n",
    "print(hyper_parameters)\n",
    "task_index"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8a63d4c0db09ff4783858371cb915982282207dafa2604e2787a0daec658aa4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dlrs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
