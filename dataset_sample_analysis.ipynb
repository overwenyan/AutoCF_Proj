{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197972"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "logfilePath = 'save/'\n",
    "random_nas_perf_dict0 = {}\n",
    "\n",
    "for num in list([1] + list(range(4,16)) + [17,16]):\n",
    "    with open(os.path.join(logfilePath, \"random_nas_perf\" + str(num) + \".json\") ,'r', encoding='UTF-8') as infile:\n",
    "        try:\n",
    "            ldict = json.load(infile)\n",
    "            # print(ldict)\n",
    "            # print()\n",
    "            random_nas_perf_dict0.update(ldict)\n",
    "        except ValueError:\n",
    "            print(infile)\n",
    "info_json = json.dumps(random_nas_perf_dict0,sort_keys=False, indent=4, separators=(',', ': '))\n",
    "f = open(os.path.join(logfilePath, \"random_nas_perf_dict0.json\"), 'w')\n",
    "f.write(info_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_recall on ml-100k:\n",
      " {'20': 0.22599699547950258, '10': 0.1394285202026367}\n",
      "\n",
      "max_arch on ml-100k:\n",
      " {'20': \"{'cf': 'rr', 'emb': {'u': 'mat', 'i': 'mat'}, 'ifc': 'mul', 'pred': 'h'}\", '10': \"{'cf': 'rr', 'emb': {'u': 'mat', 'i': 'mat'}, 'ifc': 'mul', 'pred': 'h'}\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "logfilePath = 'save/'\n",
    "random_nas_perf_dict0 = {}\n",
    "with open(os.path.join(logfilePath, \"random_nas_perf_dict0\" + \".json\") ,'r', encoding='UTF-8') as infile:\n",
    "    random_nas_perf_dict0 = json.load(infile)\n",
    "\n",
    "arch_dict_recall20 = dict()\n",
    "# arch_dict_recall10 = dict()\n",
    "max_recall = {'20': 0, '10': 0}\n",
    "max_arch = {'20': 0, '10': 0}\n",
    "arch_cnt = 0\n",
    "for k in random_nas_perf_dict0:\n",
    "    random_nas_perf_dict0[k] = np.array(random_nas_perf_dict0[k])\n",
    "    arch_cnt += 1\n",
    "    cur_max_20 = np.max(random_nas_perf_dict0[k][:,0], axis=0)\n",
    "    arch_dict_recall20[k] = np.max(np.max(random_nas_perf_dict0[k][:,0], axis=0), axis=0)\n",
    "    if cur_max_20 > max_recall['20']:\n",
    "        max_arch['20'] = k\n",
    "        max_recall['20'] = cur_max_20\n",
    "    \n",
    "    cur_max_10 = np.max(random_nas_perf_dict0[k][:,1], axis=0)\n",
    "    # print(k, cur_max_10) # recall@10\n",
    "    if cur_max_10 > max_recall['10']:\n",
    "        max_arch['10'] = k\n",
    "        max_recall['10'] = cur_max_10\n",
    "    \n",
    "\n",
    "print(\"max_recall on ml-100k:\\n {}\\n\".format(max_recall))\n",
    "print(\"max_arch on ml-100k:\\n {}\\n\".format(max_arch))\n",
    "# random_nas_perf_dict0\n",
    "# random_nas_perf_dict0\n",
    "arch_dict_recall20 = sorted(arch_dict_recall20.items(), key=lambda d:d[1], reverse = True) # 排好序的模型\n",
    "# arch_dict_recall20 = dict(arch_dict_recall20)\n",
    "# for arch, recall20 in arch_dict_recall20[:5]:\n",
    "#     print(arch, recall20)\n",
    "#     # print(\"arch_dict_recall20[:5]:\\n {}\".format(arch_dict_recall20[:5]))\n",
    "\n",
    "\n",
    "# print(arch_dict_recall20)\n",
    "info_json = json.dumps(arch_dict_recall20,sort_keys=False, indent=4, separators=(',', ': '))\n",
    "f = open(os.path.join(logfilePath, \"arch_dict_recall20_best.json\"), 'w')\n",
    "f.write(info_json)\n",
    "with open(os.path.join(logfilePath, \"ml-100k-arches-rank.txt\"), 'w') as f:\n",
    "    for arch in arch_dict_recall20:\n",
    "        # print(arch[0])\n",
    "        arch_single = eval(arch[0])\n",
    "        arch_encoding = '{}_{}_{}_{}_{}'.format(arch_single['cf'], arch_single['emb']['u'], arch_single['emb']['i'], arch_single['ifc'], arch_single['pred'])\n",
    "        # print(arch_encoding, arch[1])\n",
    "        f.writelines(arch_encoding + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arch_batch_size = 1  \n",
    "from controller import sample_arch_cf\n",
    "remaining_arches_encoding = open(\"save/ml-100k_arches.txt\", 'r').readlines() # opten the file of arch\n",
    "remaining_arches_encoding = list(map(lambda x: x.strip(), remaining_arches_encoding))\n",
    "# remaining_arches_encoding\n",
    "arches_single = []\n",
    "# arches_single\n",
    "arch_cnt = 0\n",
    "klist = [k for k in random_nas_perf_dict0]\n",
    "# print(klist[5])\n",
    "for arch_encoding in remaining_arches_encoding:\n",
    "    if arch_cnt >= len(klist):\n",
    "        break\n",
    "    arch_single = sample_arch_cf()\n",
    "    arch_single['cf'], arch_single['emb']['u'], arch_single['emb']['i'], arch_single['ifc'], arch_single['pred'] = arch_encoding.split('_')\n",
    "    arches_single.append(arch_single)\n",
    "    # print(arch_single)\n",
    "    if klist[arch_cnt] != str(arch_single):\n",
    "        print(\"arch_cnt: {}\\n arch_single: {}\\n klist[arch_cnt]: {}\\n\".format(arch_cnt,arch_single,klist[arch_cnt]))\n",
    "    arch_cnt += 1\n",
    "\n",
    "arch_cnt\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_encoding = '{}_{}_{}_{}_{}'.format(arch_single['cf'], arch_single['emb']['u'], arch_single['emb']['i'], arch_single['ifc'], arch_single['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整合单个模型的数据（GMF,MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = []\n",
    "logfilePath = 'opt_random/'\n",
    "opt_dicts = {'Adagrad': [], 'Adam': [], 'SGD': []}\n",
    "for num in [1,2,3,4,7,8]:\n",
    "    with open(os.path.join(logfilePath, \"optinfo\" + str(num) + \".json\") ,'r', encoding='UTF-8') as infile:\n",
    "        try:\n",
    "            ldict = json.load(infile)\n",
    "            for k in opt_dicts:\n",
    "                opt_dicts[k] += ldict[k]\n",
    "            \n",
    "        except ValueError:\n",
    "            print(infile)\n",
    "info_json = json.dumps(opt_dicts,sort_keys=False, indent=4, separators=(',', ': '))\n",
    "f = open(os.path.join(logfilePath, \"opt_dicts_ml-100k.json\"), 'w')\n",
    "f.write(info_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6652"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# opt ml-1m\n",
    "result = []\n",
    "logfilePath = 'opt_random/'\n",
    "opt_dicts = {'Adagrad': [], 'Adam': [], 'SGD': []}\n",
    "for num in [20,21,22,23,24,25,28,29,30,31,32,33,34,35,36,37,40]:\n",
    "    with open(os.path.join(logfilePath, \"optinfo\" + str(num) + \".json\") ,'r', encoding='UTF-8') as infile:\n",
    "        try:\n",
    "            ldict = json.load(infile)\n",
    "            for k in opt_dicts:\n",
    "                opt_dicts[k] += ldict[k]\n",
    "            result.append(ldict)\n",
    "        except ValueError:\n",
    "            print(infile)\n",
    "\n",
    "info_json = json.dumps(opt_dicts,sort_keys=False, indent=4, separators=(',', ': '))\n",
    "f = open(os.path.join(logfilePath, \"opt_dicts_ml-1m.json\"), 'w')\n",
    "f.write(info_json)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31318"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# lr ml-100k\n",
    "# result = []\n",
    "lr_hp_list = ['1e-6','1e-5', '1e-4','1e-3', '1e-2', '1e-1', '1e0','1e1','1e2', '1e3', '1e4', '1e5']\n",
    "lr_hp_list = [str(float(lr)) for lr in lr_hp_list]\n",
    "lr_dicts = {}\n",
    "for lr in lr_hp_list:\n",
    "    lr_dicts[lr] = []\n",
    "logfilePath = 'lr_random/'\n",
    "# lr_dicts = {'0.001': [], '0.005':[], '0.01':[], '0.05':[], \\\n",
    "#                     '0.1':[], '0.5':[],'1.0':[],'1.5':[],'2.0':[] ,'10.0':[], \\\n",
    "#                         '20.0': [], '50.0': []}#, '100.0':[], '1000.0':[]}\n",
    "for num in [130,131,134,135,136,137,141,142,145,146,147,148]:\n",
    "    with open(os.path.join(logfilePath, \"lrinfo\" + str(num) + \".json\") ,'r', encoding='UTF-8') as infile:\n",
    "        try:\n",
    "            ldict = json.load(infile)\n",
    "            for k in lr_dicts:\n",
    "                lr_dicts[k] += ldict[k]\n",
    "            # result.append(ldict)\n",
    "        except ValueError:\n",
    "            print(infile)\n",
    "\n",
    "info_json = json.dumps(lr_dicts,sort_keys=False, indent=4, separators=(',', ': '))\n",
    "f = open(os.path.join(logfilePath, \"lr_dicts_ml-100k_7_MLP.json\"), 'w')\n",
    "f.write(info_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32405"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "\n",
    "# lr \n",
    "cur_dataset_str = 'ml-100k'\n",
    "# result = []\n",
    "lr_hp_list = ['1e-6','1e-5', '1e-4','1e-3', '1e-2', '1e-1', '1e0','1e1','1e2', '1e3', '1e4', '1e5']\n",
    "lr_hp_list = [str(float(lr)) for lr in lr_hp_list]\n",
    "lr_dicts = {}\n",
    "for lr in lr_hp_list:\n",
    "    lr_dicts[lr] = []\n",
    "logfilePath = 'lr_random/'\n",
    "nums = [200,201,203,204,209,216,217,218,220,221,222,223,228,233,234]\n",
    "files = [os.path.join(logfilePath, \"lrinfo\" + str(num) + \".json\") for num in nums]\n",
    "\n",
    "for f in files:\n",
    "    with open(f,'r', encoding='UTF-8') as infile:\n",
    "        try:\n",
    "            ldict = json.load(infile)\n",
    "            for k in lr_dicts:\n",
    "                lr_dicts[k] += ldict[k]\n",
    "            # result.append(ldict)\n",
    "            shutil.move(f, os.path.join(logfilePath, 'lr_info_dicts')) # 移动文件\n",
    "\n",
    "        except ValueError:\n",
    "            print(infile)\n",
    "\n",
    "info_json = json.dumps(lr_dicts,sort_keys=False, indent=4, separators=(',', ': '))\n",
    "f = open(os.path.join(logfilePath, \"lr_random_nas_dicts_\"+ cur_dataset_str +\"_1.json\"), 'w')\n",
    "f.write(info_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32405"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "\n",
    "# lr \n",
    "cur_dataset_str = 'ml-1m'\n",
    "# result = []\n",
    "embs_hp_list = ['1','2', '4','8','16','32','64']\n",
    "# embgroups = np.array([[] for item in embs_hp_list],dtype='int32')\n",
    "# load_dict_list = []\n",
    "embedding_dim_result_list_dict = {}\n",
    "\n",
    "for embsize in embs_hp_list:\n",
    "     embedding_dim_result_list_dict[embsize] = []\n",
    "logfilePath = 'emb_random/'\n",
    "nums = [12,15,17,18,19,20,21,22,23,30,31,32,33,34,35,36,37,38,39]\n",
    "files = [os.path.join(logfilePath, \"embinfo\" + str(num) + \".json\") for num in nums]\n",
    "f2 = os.path.join(logfilePath, 'emb_info_dicts')\n",
    "if not os.path.exists(f2):\n",
    "    os.makedirs(f2)\n",
    "for f in files:\n",
    "    with open(f,'r', encoding='UTF-8') as infile:\n",
    "        try:\n",
    "            ldict = json.load(infile)\n",
    "            for k in embedding_dim_result_list_dict:\n",
    "                embedding_dim_result_list_dict[k] += ldict[k]\n",
    "            # result.append(ldict)\n",
    "            shutil.move(f, f2) # 移动文件\n",
    "        except ValueError:\n",
    "            print(infile)\n",
    "\n",
    "info_json = json.dumps(lr_dicts,sort_keys=False, indent=4, separators=(',', ': '))\n",
    "f = open(os.path.join(logfilePath, \"embdim_dicts_\"+ cur_dataset_str +\"_1.json\"), 'w')\n",
    "f.write(info_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32405"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "\n",
    "# lr \n",
    "cur_dataset_str = 'ml-100k'\n",
    "# result = []\n",
    "wd_hp_list = ['1e-06','1e-05', '0.0001','0.001','0.01']\n",
    "weight_decay_result_list_dict = {}\n",
    "for embsize in wd_hp_list:\n",
    "     weight_decay_result_list_dict[embsize] = []\n",
    "\n",
    "logfilePath = 'weight_random/'\n",
    "nums = [1,2,3,5,6,7]\n",
    "files = [os.path.join(logfilePath, \"weighdeinfo\" + str(num) + \".json\") for num in nums]\n",
    "f2 = os.path.join(logfilePath, 'weight_decay_info_dicts')\n",
    "if not os.path.exists(f2):\n",
    "    os.makedirs(f2)\n",
    "for f in files:\n",
    "    with open(f,'r', encoding='UTF-8') as infile:\n",
    "        try:\n",
    "            ldict = json.load(infile)\n",
    "            for k in weight_decay_result_list_dict:\n",
    "                weight_decay_result_list_dict[k] += ldict[k]\n",
    "            # result.append(ldict)\n",
    "            shutil.move(f, f2) # 移动文件\n",
    "        except ValueError:\n",
    "            print(infile)\n",
    "\n",
    "info_json = json.dumps(lr_dicts,sort_keys=False, indent=4, separators=(',', ': '))\n",
    "f = open(os.path.join(logfilePath, \"weight_decay_dicts_\"+ cur_dataset_str +\"_1.json\"), 'w')\n",
    "f.write(info_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面是`dataset.py`的分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from audioop import rms\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from itertools import product\n",
    "from time import localtime, sleep, strftime, time\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import setproctitle # to set the name of process\n",
    "import torch\n",
    "import torch.utils\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import multiprocessing as mp # 多线程工作\n",
    "\n",
    "from dataset import get_data_queue_cf, get_data_queue_cf_nonsparse, get_data_queue_efficiently, get_data_queue_negsampling_efficiently\n",
    "from models import (CML, DELF, DMF, FISM, GMF, MLP, SVD, JNCF_Cat, JNCF_Dot, SVD_plus_plus, SPACE, BaseModel, Virtue_CF)\n",
    "from controller import sample_arch_cf, sample_arch_cf_signal, sample_arch_cf_test\n",
    "from train_eval import (evaluate_cf, evaluate_cf_efficiently, evaluate_cf_efficiently_implicit, get_arch_performance_cf_signal_param_device, get_arch_performance_single_device, train_single_cf, train_single_cf_efficiently,get_arch_performance_implicit_single_device)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import GPUtil\n",
    "import socket\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Run.\")\n",
    "parser.add_argument('--lr', type=float, default=0.05, help='init learning rate')\n",
    "parser.add_argument('--arch_lr', type=float, default=0.05, help='learning rate for arch encoding')\n",
    "parser.add_argument('--controller_lr', type=float, default=1e-1, help='learning rate for controller')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-5, help='weight decay')\n",
    "parser.add_argument('--update_freq', type=int, default=1, help='frequency of updating architeture')\n",
    "parser.add_argument('--opt', type=str, default='Adagrad', help='choice of opt')\n",
    "parser.add_argument('--use_gpu', type=int, default=1, help='whether use gpu')\n",
    "parser.add_argument('--minibatch', type=int, default=1, help='whether use minibatch')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--train_epochs', type=int, default=2000, help='num of training epochs')\n",
    "parser.add_argument('--search_epochs', type=int, default=1000, help='num of searching epochs')\n",
    "parser.add_argument('--save', type=str, default='save/', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "parser.add_argument('--train_portion', type=float, default=0.5, help='portion of training data')\n",
    "parser.add_argument('--valid_portion', type=float, default=0.25, help='portion of validation data')\n",
    "parser.add_argument('--dataset', type=str, default='ml-100k', help='dataset')\n",
    "parser.add_argument('--mode', type=str, default='random_single', help='search or single mode')\n",
    "parser.add_argument('--process_name', type=str, default='AutoCF@wenyan', help='process name')\n",
    "parser.add_argument('--embedding_dim', type=int, default=2, help='dimension of embedding')\n",
    "parser.add_argument('--controller', type=str, default='PURE', help='structure of controller')\n",
    "parser.add_argument('--controller_batch_size', type=int, default=4, help='batch size for updating controller')\n",
    "parser.add_argument('--unrolled', action='store_true', default=True, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--max_batch', type=int, default=65536, help='max batch during training')\n",
    "parser.add_argument('--device', type=int, default=0, help='GPU device')\n",
    "parser.add_argument('--multi', type=int, default=0, help='using multi-training for single architecture')\n",
    "parser.add_argument('--if_valid', type=int, default=1, help='use validation set for tuning single architecture or not')\n",
    "parser.add_argument('--breakpoint', type=str, default='save/log.txt', help='the log file storing existing results')\n",
    "parser.add_argument('--arch_file', type=str, default='src/arch.txt', help='all arches')\n",
    "parser.add_argument('--remaining_arches', type=str, default='src/arch.txt', help='')\n",
    "parser.add_argument('--arch_assign', type=str, default='[0,3]', help='')\n",
    "parser.add_argument('--data_type', type=str, default='implicit', help='explicit or implicit(default)')\n",
    "parser.add_argument('--loss_func', type=str, default='bprloss', help='Implicit loss function')\n",
    "parser.add_argument('--mark', type=str, default='') # \n",
    "\n",
    "args = parser.parse_args([])\n",
    "mp.set_start_method('spawn', force=True) # 一种多任务运行方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/09 02:45:01 PM gpu device = 0\n",
      "03/09 02:45:03 PM prepare data finish! [1.618201]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "setproctitle.setproctitle(args.process_name) # 设定进程名称\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "log_format = '%(asctime)s %(message)s' # 记录精确的实践\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, filemode='w', format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "current_time = strftime(\"%Y-%m-%d-%H:%M:%S\", localtime())\n",
    "# args.save = 'save/'\n",
    "save_name = args.mode + '_' + args.dataset + '_' + str(args.embedding_dim) + '_' + args.opt + str(args.lr)\n",
    "save_name += '_' + str(args.data_type)\n",
    "\n",
    "if args.mode == 'reinforce':\n",
    "    save_name += '_' + str(args.controller_lr) + '_' + args.controller + '_' + str(args.controller_batch_size)\n",
    "else:\n",
    "    # save_name += '_' + str(args.weight_decay) # default=1e-5\n",
    "    save_name += '_' + ('%.6f' % (args.weight_decay)) \n",
    "save_name += '_' + str(args.seed) # default=1\n",
    "save_name += '_' + current_time\n",
    "# save_name = args.mode + '_' + args.dataset + '_' + str(args.embedding_dim) + '_' \n",
    "# + args.opt + str(args.lr) + '_' + str(args.weight_decay) + '_' \n",
    "# + str(args.seed) + '_' + current_time\n",
    "\n",
    "# 创建log路径\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "if not os.path.exists(args.save + '/log'):\n",
    "    os.makedirs(args.save + '/log')\n",
    "if not os.path.exists(args.save + '/log_sub'):\n",
    "    os.makedirs(args.save + '/log_sub')\n",
    "if os.path.exists(os.path.join(args.save, save_name + '.txt')):\n",
    "    os.remove(os.path.join(args.save, save_name + '.txt'))\n",
    "\n",
    "# fh表示\n",
    "fh = logging.FileHandler(os.path.join(args.save + 'log', save_name + '.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "writer = SummaryWriter(log_dir=args.save + 'tensorboard/{}'.format(save_name))\n",
    "if args.use_gpu: # default = True\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    logging.info('gpu device = %d' % args.gpu)\n",
    "else:\n",
    "    logging.info('no gpu')\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "data_start = time()\n",
    "dim = 2\n",
    "data_path = args.dataset + '/'\n",
    "\n",
    "# setting datasets,  default='ml-100k'\n",
    "if args.dataset == 'ml-100k': # default\n",
    "    num_users = 943\n",
    "    num_items = 1682\n",
    "elif args.dataset == 'ml-1m':\n",
    "    num_users = 6040\n",
    "    num_items = 3952\n",
    "elif args.dataset == 'ml-10m':\n",
    "    # num_users = 71567\n",
    "    # num_items = 65133\n",
    "    num_users = 715\n",
    "    num_items = 653\n",
    "elif args.dataset == 'ml-20m':\n",
    "    num_users = 138493\n",
    "    num_items = 131262\n",
    "elif args.dataset == 'youtube_small':\n",
    "    num_ps = 600\n",
    "    num_qs = 14340\n",
    "    num_rs = 5\n",
    "    dim = 3\n",
    "elif args.dataset == 'youtube':\n",
    "    num_ps = 15088\n",
    "    num_qs = 15088\n",
    "    num_rs = 5\n",
    "    dim = 3\n",
    "elif args.dataset == 'amazon-book':\n",
    "    num_users = 11899\n",
    "    num_items = 16196\n",
    "elif args.dataset == 'yelp':\n",
    "    num_users = 26829\n",
    "    num_items = 20344\n",
    "elif args.dataset == 'yelp2':\n",
    "    num_users = 15496\n",
    "    num_items = 12666\n",
    "else:\n",
    "    pass\n",
    "args.num_users = num_users\n",
    "args.num_items = num_items\n",
    "\n",
    "if args.data_type == 'implicit': # 主要使用这一行，隐式推荐\n",
    "    train_queue_pair, valid_queue, test_queue = get_data_queue_negsampling_efficiently(data_path, args)\n",
    "else: # train queue，显式推荐\n",
    "    train_queue, valid_queue, test_queue = get_data_queue_efficiently(data_path, args)\n",
    "# print(train_queue)\n",
    "logging.info('prepare data finish! [%f]' % (time()-data_start))\n",
    "stored_arches = {} # log ging表示添加到记录中\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_queue_pair.len: 5, 50000\n",
      "valid_queue.len: 5, 25000\n",
      "test_queue.len: 5, 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"train_queue_pair.len: {}, {}\".format(len(train_queue_pair), train_queue_pair[0].shape[0]))\n",
    "print(\"valid_queue.len: {}, {}\".format(len(valid_queue), valid_queue[0].shape[0]))\n",
    "print(\"test_queue.len: {}, {}\".format(len(test_queue), test_queue[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_queue_pair[0].shape: torch.Size([50000])\n",
      "train_queue_pair[1].shape: torch.Size([50000])\n",
      "train_queue_pair[2].shape: torch.Size([50000])\n",
      "train_queue_pair[3].shape: torch.Size([943, 1682])\n",
      "train_queue_pair[4].shape: torch.Size([1682, 943])\n",
      "\n",
      "valid_queue[0].shape: torch.Size([25000])\n",
      "valid_queue[1].shape: torch.Size([25000])\n",
      "valid_queue[2].shape: torch.Size([25000])\n",
      "valid_queue[3].shape: torch.Size([943, 1682])\n",
      "valid_queue[4].shape: torch.Size([1682, 943])\n",
      "\n",
      "test_queue[0].shape: torch.Size([25000])\n",
      "test_queue[1].shape: torch.Size([25000])\n",
      "test_queue[2].shape: torch.Size([943, 1682])\n",
      "test_queue[3].shape: torch.Size([943, 1682])\n",
      "test_queue[4].shape: torch.Size([1682, 943])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_queue_pair)):\n",
    "    print(\"train_queue_pair[{}].shape: {}\".format(i, train_queue_pair[i].shape))\n",
    "print()\n",
    "for i in range(len(valid_queue)):\n",
    "    print(\"valid_queue[{}].shape: {}\".format(i, valid_queue[i].shape))\n",
    "print()\n",
    "for i in range(len(test_queue)):\n",
    "    print(\"test_queue[{}].shape: {}\".format(i, test_queue[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 [689, 832, 15, 444, 415]\n",
      "100000 [1041, 640, 947, 830, 125]\n",
      "100000 [4.0, 4.0, 3.0, 1.0, 5.0]\n",
      "num_train: 50000, num_valid: 25000\n",
      "num_users: 943, num_items: 1682\n",
      "torch.Size([943, 1682])\n",
      "torch.Size([1682, 943])\n"
     ]
    }
   ],
   "source": [
    "'''implicit数据集组织方法'''\n",
    "data_path = args.dataset + '/'\n",
    "\n",
    "users, items, labels = [], [], []\n",
    "if args.dataset == 'ml-100k':\n",
    "    data_path += 'u.data'\n",
    "else:\n",
    "    pass\n",
    "\n",
    "data_path = 'data/' + data_path\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        # e.g. line = 196\t242\t3\t881250949\n",
    "        if args.dataset == 'ml-100k':\n",
    "            line = line.split()\n",
    "        elif args.dataset == 'ml-1m' or args.dataset == 'ml-10m':\n",
    "            line = line.split('::')\n",
    "        elif args.dataset == 'ml-20m':\n",
    "            if i == 0:\n",
    "                continue\n",
    "            line = line.split(',')\n",
    "        elif args.dataset == 'amazon-book':\n",
    "            line = line.split(',')\n",
    "        elif args.dataset == 'yelp' or 'yelp2':\n",
    "            line = line.split(',')\n",
    "        user = int(line[0]) - 1 if args.dataset != 'amazon-book' and args.dataset != 'yelp' and args.dataset != 'yelp2' else int(line[0])\n",
    "        item = int(line[1]) - 1 if args.dataset != 'amazon-book' and args.dataset != 'yelp' and args.dataset != 'yelp2' else int(line[1])\n",
    "        label = float(line[2])\n",
    "        users.append(user)\n",
    "        items.append(item)\n",
    "        labels.append(label)\n",
    "users, items, labels = shuffle(users, items, labels)\n",
    "\n",
    "print(len(users), users[0:5])\n",
    "print(len(items), items[0:5])\n",
    "print(len(labels), labels[0:5])\n",
    "num_train = int(len(users) * args.train_portion)\n",
    "num_valid = int(len(users) * args.valid_portion)\n",
    "print(\"num_train: {}, num_valid: {}\".format(num_train, num_valid))\n",
    "users_train = np.array(users[:num_train], dtype=np.int32)\n",
    "items_train = np.array(items[:num_train], dtype=np.int32)\n",
    "labels_train = np.array(labels[:num_train], dtype=np.float32)\n",
    "\n",
    "num_users = max(users) + 1\n",
    "num_items = max(items) + 1\n",
    "print(\"num_users: {}, num_items: {}\".format(num_users,num_items))\n",
    "user_interactions = torch.from_numpy(sp.coo_matrix(\n",
    "    (labels_train, (users_train, items_train)), shape=(num_users, num_items)).tocsr().toarray())\n",
    "item_interactions = torch.from_numpy(sp.coo_matrix(\n",
    "    (labels_train, (items_train, users_train)), shape=(num_items, num_users)).tocsr().toarray())\n",
    "a = time.time()\n",
    "print(user_interactions.shape)\n",
    "print(item_interactions.shape)\n",
    "# print(user_interactions)\n",
    "# print(item_interactions.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negs_train: tensor([1541, 1065,  201,  ..., 1173, 1647,  259]), negs_train.shape: torch.Size([50000])\n",
      "items_train: [1041  640  947 ...   69  107 1102], items_train.shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "negs_train = np.zeros(len(labels_train), dtype=np.int64)\n",
    "for k in range(len(labels_train)):\n",
    "    neg = np.random.randint(num_items)\n",
    "    # print(neg)\n",
    "    while user_interactions[users_train[k], neg] != 0:\n",
    "        neg = np.random.randint(num_items)\n",
    "    negs_train[k] = neg\n",
    "negs_train = torch.from_numpy(negs_train)\n",
    "print(\"negs_train: {}, negs_train.shape: {}\".format(negs_train, negs_train.shape))\n",
    "print(\"items_train: {}, items_train.shape: {}\".format(items_train, items_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([943, 1682]) 25000\n"
     ]
    }
   ],
   "source": [
    "train_queue_pair = [torch.tensor(users[:num_train]),\n",
    "                    torch.tensor(items[:num_train]),\n",
    "                    negs_train, # 特有的\n",
    "                    user_interactions, item_interactions]\n",
    "\n",
    "valid_queue = [torch.tensor(users[num_train:num_train+num_valid]),\n",
    "                torch.tensor(items[num_train:num_train+num_valid]),\n",
    "                torch.tensor(labels[num_train:num_train+num_valid]),\n",
    "                user_interactions, item_interactions]\n",
    "\n",
    "users_test = np.array(users[num_train+num_valid:], dtype=np.int32)\n",
    "items_test = np.array(items[num_train+num_valid:], dtype=np.int32)\n",
    "labels_test = np.array(labels[num_train+num_valid:], dtype=np.float32)\n",
    "test_user_interactions = torch.from_numpy(sp.coo_matrix(\n",
    "    (labels_test, (users_test, items_test)), shape=(num_users, num_items)).tocsr().toarray())\n",
    "print(test_user_interactions.shape, np.count_nonzero(test_user_interactions))\n",
    "\n",
    "a = np.argsort(users[num_train+num_valid:])\n",
    "test_queue = [torch.tensor(np.array(users[num_train+num_valid:], dtype=np.int64)[a]),\n",
    "                torch.tensor(np.array(items[num_train+num_valid:], dtype=np.int64)[a]),\n",
    "                test_user_interactions,\n",
    "                user_interactions, item_interactions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([943, 1682]) 25000\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8a63d4c0db09ff4783858371cb915982282207dafa2604e2787a0daec658aa4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dlrs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
